{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9Xqk7R4pMuA",
        "outputId": "165c793d-2a5c-41b7-8f14-8caab82514f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A= 1600\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.313417  [   16/60000]\n",
            "loss: 2.291655  [ 1616/60000]\n",
            "loss: 2.278991  [ 3216/60000]\n",
            "loss: 2.313117  [ 4816/60000]\n",
            "loss: 2.267166  [ 6416/60000]\n",
            "loss: 2.270435  [ 8016/60000]\n",
            "loss: 2.247584  [ 9616/60000]\n",
            "loss: 2.209974  [11216/60000]\n",
            "loss: 2.301935  [12816/60000]\n",
            "loss: 2.281051  [14416/60000]\n",
            "loss: 2.229752  [16016/60000]\n",
            "loss: 2.217820  [17616/60000]\n",
            "loss: 2.228741  [19216/60000]\n",
            "loss: 2.213940  [20816/60000]\n",
            "loss: 2.181146  [22416/60000]\n",
            "loss: 2.119844  [24016/60000]\n",
            "loss: 2.118382  [25616/60000]\n",
            "loss: 2.151251  [27216/60000]\n",
            "loss: 2.081959  [28816/60000]\n",
            "loss: 2.049942  [30416/60000]\n",
            "loss: 1.992893  [32016/60000]\n",
            "loss: 1.868909  [33616/60000]\n",
            "loss: 1.796808  [35216/60000]\n",
            "loss: 1.863489  [36816/60000]\n",
            "loss: 1.645763  [38416/60000]\n",
            "loss: 1.703455  [40016/60000]\n",
            "loss: 1.605142  [41616/60000]\n",
            "loss: 1.547074  [43216/60000]\n",
            "loss: 1.340320  [44816/60000]\n",
            "loss: 1.420938  [46416/60000]\n",
            "loss: 1.403705  [48016/60000]\n",
            "loss: 1.347650  [49616/60000]\n",
            "loss: 1.286943  [51216/60000]\n",
            "loss: 1.079656  [52816/60000]\n",
            "loss: 1.492357  [54416/60000]\n",
            "loss: 1.502306  [56016/60000]\n",
            "loss: 1.295212  [57616/60000]\n",
            "loss: 0.905972  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 59.9%, Avg loss: 1.077853 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.104122  [   16/60000]\n",
            "loss: 1.187394  [ 1616/60000]\n",
            "loss: 0.946751  [ 3216/60000]\n",
            "loss: 0.982153  [ 4816/60000]\n",
            "loss: 0.871772  [ 6416/60000]\n",
            "loss: 1.114389  [ 8016/60000]\n",
            "loss: 0.994773  [ 9616/60000]\n",
            "loss: 0.916193  [11216/60000]\n",
            "loss: 0.909158  [12816/60000]\n",
            "loss: 1.159730  [14416/60000]\n",
            "loss: 1.157538  [16016/60000]\n",
            "loss: 0.940520  [17616/60000]\n",
            "loss: 0.902676  [19216/60000]\n",
            "loss: 1.487407  [20816/60000]\n",
            "loss: 0.976942  [22416/60000]\n",
            "loss: 1.219776  [24016/60000]\n",
            "loss: 0.839181  [25616/60000]\n",
            "loss: 1.517904  [27216/60000]\n",
            "loss: 1.483459  [28816/60000]\n",
            "loss: 1.059141  [30416/60000]\n",
            "loss: 1.024887  [32016/60000]\n",
            "loss: 0.452402  [33616/60000]\n",
            "loss: 0.661166  [35216/60000]\n",
            "loss: 0.913759  [36816/60000]\n",
            "loss: 1.288842  [38416/60000]\n",
            "loss: 0.694800  [40016/60000]\n",
            "loss: 0.814635  [41616/60000]\n",
            "loss: 0.916139  [43216/60000]\n",
            "loss: 0.623018  [44816/60000]\n",
            "loss: 0.793106  [46416/60000]\n",
            "loss: 0.984578  [48016/60000]\n",
            "loss: 0.706060  [49616/60000]\n",
            "loss: 0.770676  [51216/60000]\n",
            "loss: 0.840982  [52816/60000]\n",
            "loss: 0.999075  [54416/60000]\n",
            "loss: 1.154795  [56016/60000]\n",
            "loss: 0.904754  [57616/60000]\n",
            "loss: 0.648382  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.5%, Avg loss: 0.833342 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.795934  [   16/60000]\n",
            "loss: 0.867201  [ 1616/60000]\n",
            "loss: 0.835365  [ 3216/60000]\n",
            "loss: 0.942076  [ 4816/60000]\n",
            "loss: 0.805571  [ 6416/60000]\n",
            "loss: 1.233873  [ 8016/60000]\n",
            "loss: 1.115243  [ 9616/60000]\n",
            "loss: 0.739627  [11216/60000]\n",
            "loss: 0.932205  [12816/60000]\n",
            "loss: 0.996058  [14416/60000]\n",
            "loss: 1.127547  [16016/60000]\n",
            "loss: 0.677536  [17616/60000]\n",
            "loss: 0.608768  [19216/60000]\n",
            "loss: 1.564822  [20816/60000]\n",
            "loss: 0.665005  [22416/60000]\n",
            "loss: 1.053305  [24016/60000]\n",
            "loss: 0.685042  [25616/60000]\n",
            "loss: 1.324601  [27216/60000]\n",
            "loss: 1.141544  [28816/60000]\n",
            "loss: 0.782213  [30416/60000]\n",
            "loss: 0.824220  [32016/60000]\n",
            "loss: 0.430499  [33616/60000]\n",
            "loss: 0.609909  [35216/60000]\n",
            "loss: 0.703129  [36816/60000]\n",
            "loss: 0.985539  [38416/60000]\n",
            "loss: 0.923054  [40016/60000]\n",
            "loss: 0.894848  [41616/60000]\n",
            "loss: 0.877766  [43216/60000]\n",
            "loss: 0.436689  [44816/60000]\n",
            "loss: 0.660585  [46416/60000]\n",
            "loss: 0.802471  [48016/60000]\n",
            "loss: 0.618792  [49616/60000]\n",
            "loss: 0.549478  [51216/60000]\n",
            "loss: 0.748986  [52816/60000]\n",
            "loss: 0.774210  [54416/60000]\n",
            "loss: 1.130193  [56016/60000]\n",
            "loss: 0.982127  [57616/60000]\n",
            "loss: 0.598424  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 72.4%, Avg loss: 0.750337 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.622517  [   16/60000]\n",
            "loss: 0.835574  [ 1616/60000]\n",
            "loss: 0.560914  [ 3216/60000]\n",
            "loss: 0.768285  [ 4816/60000]\n",
            "loss: 0.660754  [ 6416/60000]\n",
            "loss: 1.252347  [ 8016/60000]\n",
            "loss: 0.919875  [ 9616/60000]\n",
            "loss: 0.562457  [11216/60000]\n",
            "loss: 0.791896  [12816/60000]\n",
            "loss: 0.770694  [14416/60000]\n",
            "loss: 1.012479  [16016/60000]\n",
            "loss: 0.667918  [17616/60000]\n",
            "loss: 0.480595  [19216/60000]\n",
            "loss: 1.337046  [20816/60000]\n",
            "loss: 0.567188  [22416/60000]\n",
            "loss: 1.006806  [24016/60000]\n",
            "loss: 0.867934  [25616/60000]\n",
            "loss: 0.956274  [27216/60000]\n",
            "loss: 1.212879  [28816/60000]\n",
            "loss: 0.726095  [30416/60000]\n",
            "loss: 0.773958  [32016/60000]\n",
            "loss: 0.296888  [33616/60000]\n",
            "loss: 0.466617  [35216/60000]\n",
            "loss: 0.567259  [36816/60000]\n",
            "loss: 0.830623  [38416/60000]\n",
            "loss: 0.659524  [40016/60000]\n",
            "loss: 0.908372  [41616/60000]\n",
            "loss: 0.780434  [43216/60000]\n",
            "loss: 0.553692  [44816/60000]\n",
            "loss: 0.686442  [46416/60000]\n",
            "loss: 0.729270  [48016/60000]\n",
            "loss: 0.479307  [49616/60000]\n",
            "loss: 0.618959  [51216/60000]\n",
            "loss: 0.692404  [52816/60000]\n",
            "loss: 0.777644  [54416/60000]\n",
            "loss: 0.913417  [56016/60000]\n",
            "loss: 0.842925  [57616/60000]\n",
            "loss: 0.685581  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.8%, Avg loss: 0.698375 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.569848  [   16/60000]\n",
            "loss: 0.625613  [ 1616/60000]\n",
            "loss: 0.575902  [ 3216/60000]\n",
            "loss: 0.647609  [ 4816/60000]\n",
            "loss: 0.658227  [ 6416/60000]\n",
            "loss: 0.986096  [ 8016/60000]\n",
            "loss: 0.883950  [ 9616/60000]\n",
            "loss: 0.603899  [11216/60000]\n",
            "loss: 0.587498  [12816/60000]\n",
            "loss: 0.625530  [14416/60000]\n",
            "loss: 0.744686  [16016/60000]\n",
            "loss: 0.706413  [17616/60000]\n",
            "loss: 0.586123  [19216/60000]\n",
            "loss: 1.194570  [20816/60000]\n",
            "loss: 0.536254  [22416/60000]\n",
            "loss: 0.834374  [24016/60000]\n",
            "loss: 0.710639  [25616/60000]\n",
            "loss: 0.886567  [27216/60000]\n",
            "loss: 0.903356  [28816/60000]\n",
            "loss: 0.711628  [30416/60000]\n",
            "loss: 0.742895  [32016/60000]\n",
            "loss: 0.344651  [33616/60000]\n",
            "loss: 0.482432  [35216/60000]\n",
            "loss: 0.589501  [36816/60000]\n",
            "loss: 0.815602  [38416/60000]\n",
            "loss: 0.610228  [40016/60000]\n",
            "loss: 0.708383  [41616/60000]\n",
            "loss: 0.758970  [43216/60000]\n",
            "loss: 0.538480  [44816/60000]\n",
            "loss: 0.682879  [46416/60000]\n",
            "loss: 0.758088  [48016/60000]\n",
            "loss: 0.488648  [49616/60000]\n",
            "loss: 0.435486  [51216/60000]\n",
            "loss: 0.782216  [52816/60000]\n",
            "loss: 0.827914  [54416/60000]\n",
            "loss: 0.801542  [56016/60000]\n",
            "loss: 0.936909  [57616/60000]\n",
            "loss: 0.426853  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 74.9%, Avg loss: 0.659096 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.741085  [   16/60000]\n",
            "loss: 0.514114  [ 1616/60000]\n",
            "loss: 0.552336  [ 3216/60000]\n",
            "loss: 0.704323  [ 4816/60000]\n",
            "loss: 0.517798  [ 6416/60000]\n",
            "loss: 0.887753  [ 8016/60000]\n",
            "loss: 0.748538  [ 9616/60000]\n",
            "loss: 0.553533  [11216/60000]\n",
            "loss: 0.481041  [12816/60000]\n",
            "loss: 0.431635  [14416/60000]\n",
            "loss: 1.044301  [16016/60000]\n",
            "loss: 0.643153  [17616/60000]\n",
            "loss: 0.385785  [19216/60000]\n",
            "loss: 1.212984  [20816/60000]\n",
            "loss: 0.675158  [22416/60000]\n",
            "loss: 0.820936  [24016/60000]\n",
            "loss: 0.503925  [25616/60000]\n",
            "loss: 0.928933  [27216/60000]\n",
            "loss: 1.082114  [28816/60000]\n",
            "loss: 0.670693  [30416/60000]\n",
            "loss: 0.723816  [32016/60000]\n",
            "loss: 0.221450  [33616/60000]\n",
            "loss: 0.449436  [35216/60000]\n",
            "loss: 0.583287  [36816/60000]\n",
            "loss: 1.013583  [38416/60000]\n",
            "loss: 0.756896  [40016/60000]\n",
            "loss: 0.721531  [41616/60000]\n",
            "loss: 0.840791  [43216/60000]\n",
            "loss: 0.441147  [44816/60000]\n",
            "loss: 0.542472  [46416/60000]\n",
            "loss: 0.588363  [48016/60000]\n",
            "loss: 0.418065  [49616/60000]\n",
            "loss: 0.408871  [51216/60000]\n",
            "loss: 0.573174  [52816/60000]\n",
            "loss: 0.466187  [54416/60000]\n",
            "loss: 0.754365  [56016/60000]\n",
            "loss: 0.988575  [57616/60000]\n",
            "loss: 0.358534  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.7%, Avg loss: 0.630226 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.787748  [   16/60000]\n",
            "loss: 0.624162  [ 1616/60000]\n",
            "loss: 0.509272  [ 3216/60000]\n",
            "loss: 0.632019  [ 4816/60000]\n",
            "loss: 0.488232  [ 6416/60000]\n",
            "loss: 0.845692  [ 8016/60000]\n",
            "loss: 0.728914  [ 9616/60000]\n",
            "loss: 0.399792  [11216/60000]\n",
            "loss: 0.588653  [12816/60000]\n",
            "loss: 0.431243  [14416/60000]\n",
            "loss: 0.756882  [16016/60000]\n",
            "loss: 0.585492  [17616/60000]\n",
            "loss: 0.369461  [19216/60000]\n",
            "loss: 1.071689  [20816/60000]\n",
            "loss: 0.579637  [22416/60000]\n",
            "loss: 1.077586  [24016/60000]\n",
            "loss: 0.639597  [25616/60000]\n",
            "loss: 0.930423  [27216/60000]\n",
            "loss: 1.127151  [28816/60000]\n",
            "loss: 0.562396  [30416/60000]\n",
            "loss: 0.856550  [32016/60000]\n",
            "loss: 0.311490  [33616/60000]\n",
            "loss: 0.313154  [35216/60000]\n",
            "loss: 0.526545  [36816/60000]\n",
            "loss: 0.821182  [38416/60000]\n",
            "loss: 0.618583  [40016/60000]\n",
            "loss: 0.722069  [41616/60000]\n",
            "loss: 0.647674  [43216/60000]\n",
            "loss: 0.483749  [44816/60000]\n",
            "loss: 0.609583  [46416/60000]\n",
            "loss: 0.534191  [48016/60000]\n",
            "loss: 0.346821  [49616/60000]\n",
            "loss: 0.423389  [51216/60000]\n",
            "loss: 0.680777  [52816/60000]\n",
            "loss: 0.546384  [54416/60000]\n",
            "loss: 0.581337  [56016/60000]\n",
            "loss: 0.838323  [57616/60000]\n",
            "loss: 0.519346  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.4%, Avg loss: 0.604962 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.592140  [   16/60000]\n",
            "loss: 0.612038  [ 1616/60000]\n",
            "loss: 0.446054  [ 3216/60000]\n",
            "loss: 0.504003  [ 4816/60000]\n",
            "loss: 0.533066  [ 6416/60000]\n",
            "loss: 0.915931  [ 8016/60000]\n",
            "loss: 0.618266  [ 9616/60000]\n",
            "loss: 0.537049  [11216/60000]\n",
            "loss: 0.549329  [12816/60000]\n",
            "loss: 0.383116  [14416/60000]\n",
            "loss: 0.948147  [16016/60000]\n",
            "loss: 0.552498  [17616/60000]\n",
            "loss: 0.351418  [19216/60000]\n",
            "loss: 1.083829  [20816/60000]\n",
            "loss: 0.448774  [22416/60000]\n",
            "loss: 0.909582  [24016/60000]\n",
            "loss: 0.651375  [25616/60000]\n",
            "loss: 0.882041  [27216/60000]\n",
            "loss: 1.067930  [28816/60000]\n",
            "loss: 0.546068  [30416/60000]\n",
            "loss: 0.662951  [32016/60000]\n",
            "loss: 0.221820  [33616/60000]\n",
            "loss: 0.371474  [35216/60000]\n",
            "loss: 0.537315  [36816/60000]\n",
            "loss: 0.496960  [38416/60000]\n",
            "loss: 0.682311  [40016/60000]\n",
            "loss: 0.548309  [41616/60000]\n",
            "loss: 0.654369  [43216/60000]\n",
            "loss: 0.384465  [44816/60000]\n",
            "loss: 0.523514  [46416/60000]\n",
            "loss: 0.579513  [48016/60000]\n",
            "loss: 0.429678  [49616/60000]\n",
            "loss: 0.373673  [51216/60000]\n",
            "loss: 0.452909  [52816/60000]\n",
            "loss: 0.567389  [54416/60000]\n",
            "loss: 0.561310  [56016/60000]\n",
            "loss: 0.754398  [57616/60000]\n",
            "loss: 0.439315  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.583128 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.510087  [   16/60000]\n",
            "loss: 0.584699  [ 1616/60000]\n",
            "loss: 0.558236  [ 3216/60000]\n",
            "loss: 0.526536  [ 4816/60000]\n",
            "loss: 0.555722  [ 6416/60000]\n",
            "loss: 0.758178  [ 8016/60000]\n",
            "loss: 0.627437  [ 9616/60000]\n",
            "loss: 0.461487  [11216/60000]\n",
            "loss: 0.415456  [12816/60000]\n",
            "loss: 0.376677  [14416/60000]\n",
            "loss: 0.925704  [16016/60000]\n",
            "loss: 0.602758  [17616/60000]\n",
            "loss: 0.357181  [19216/60000]\n",
            "loss: 0.708791  [20816/60000]\n",
            "loss: 0.464048  [22416/60000]\n",
            "loss: 0.875651  [24016/60000]\n",
            "loss: 0.633710  [25616/60000]\n",
            "loss: 0.806239  [27216/60000]\n",
            "loss: 1.325252  [28816/60000]\n",
            "loss: 0.489285  [30416/60000]\n",
            "loss: 0.545468  [32016/60000]\n",
            "loss: 0.260986  [33616/60000]\n",
            "loss: 0.387573  [35216/60000]\n",
            "loss: 0.543548  [36816/60000]\n",
            "loss: 0.831751  [38416/60000]\n",
            "loss: 0.748138  [40016/60000]\n",
            "loss: 0.805112  [41616/60000]\n",
            "loss: 0.764046  [43216/60000]\n",
            "loss: 0.512202  [44816/60000]\n",
            "loss: 0.618498  [46416/60000]\n",
            "loss: 0.557732  [48016/60000]\n",
            "loss: 0.426316  [49616/60000]\n",
            "loss: 0.362586  [51216/60000]\n",
            "loss: 0.541102  [52816/60000]\n",
            "loss: 0.582823  [54416/60000]\n",
            "loss: 0.582354  [56016/60000]\n",
            "loss: 0.756081  [57616/60000]\n",
            "loss: 0.503251  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.8%, Avg loss: 0.565625 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.487190  [   16/60000]\n",
            "loss: 0.298390  [ 1616/60000]\n",
            "loss: 0.462734  [ 3216/60000]\n",
            "loss: 0.521800  [ 4816/60000]\n",
            "loss: 0.581537  [ 6416/60000]\n",
            "loss: 0.694622  [ 8016/60000]\n",
            "loss: 0.653540  [ 9616/60000]\n",
            "loss: 0.346032  [11216/60000]\n",
            "loss: 0.412950  [12816/60000]\n",
            "loss: 0.376834  [14416/60000]\n",
            "loss: 0.870143  [16016/60000]\n",
            "loss: 0.487552  [17616/60000]\n",
            "loss: 0.337522  [19216/60000]\n",
            "loss: 0.840418  [20816/60000]\n",
            "loss: 0.402702  [22416/60000]\n",
            "loss: 0.615988  [24016/60000]\n",
            "loss: 0.556140  [25616/60000]\n",
            "loss: 0.926982  [27216/60000]\n",
            "loss: 1.023330  [28816/60000]\n",
            "loss: 0.615328  [30416/60000]\n",
            "loss: 0.735957  [32016/60000]\n",
            "loss: 0.300184  [33616/60000]\n",
            "loss: 0.406838  [35216/60000]\n",
            "loss: 0.529825  [36816/60000]\n",
            "loss: 0.540929  [38416/60000]\n",
            "loss: 0.543495  [40016/60000]\n",
            "loss: 0.765248  [41616/60000]\n",
            "loss: 0.458277  [43216/60000]\n",
            "loss: 0.458966  [44816/60000]\n",
            "loss: 0.563602  [46416/60000]\n",
            "loss: 0.575871  [48016/60000]\n",
            "loss: 0.416908  [49616/60000]\n",
            "loss: 0.378026  [51216/60000]\n",
            "loss: 0.522749  [52816/60000]\n",
            "loss: 0.611639  [54416/60000]\n",
            "loss: 0.564157  [56016/60000]\n",
            "loss: 0.679132  [57616/60000]\n",
            "loss: 0.472525  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.5%, Avg loss: 0.549596 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.535529  [   16/60000]\n",
            "loss: 0.589787  [ 1616/60000]\n",
            "loss: 0.601978  [ 3216/60000]\n",
            "loss: 0.521876  [ 4816/60000]\n",
            "loss: 0.623447  [ 6416/60000]\n",
            "loss: 0.851040  [ 8016/60000]\n",
            "loss: 0.586701  [ 9616/60000]\n",
            "loss: 0.428523  [11216/60000]\n",
            "loss: 0.381562  [12816/60000]\n",
            "loss: 0.275949  [14416/60000]\n",
            "loss: 0.973184  [16016/60000]\n",
            "loss: 0.394283  [17616/60000]\n",
            "loss: 0.304381  [19216/60000]\n",
            "loss: 0.659662  [20816/60000]\n",
            "loss: 0.414834  [22416/60000]\n",
            "loss: 0.804238  [24016/60000]\n",
            "loss: 0.486223  [25616/60000]\n",
            "loss: 0.850295  [27216/60000]\n",
            "loss: 1.047760  [28816/60000]\n",
            "loss: 0.547352  [30416/60000]\n",
            "loss: 0.470947  [32016/60000]\n",
            "loss: 0.289435  [33616/60000]\n",
            "loss: 0.345251  [35216/60000]\n",
            "loss: 0.617913  [36816/60000]\n",
            "loss: 0.514141  [38416/60000]\n",
            "loss: 0.671959  [40016/60000]\n",
            "loss: 0.716649  [41616/60000]\n",
            "loss: 0.738723  [43216/60000]\n",
            "loss: 0.438018  [44816/60000]\n",
            "loss: 0.437138  [46416/60000]\n",
            "loss: 0.374264  [48016/60000]\n",
            "loss: 0.520648  [49616/60000]\n",
            "loss: 0.473565  [51216/60000]\n",
            "loss: 0.429082  [52816/60000]\n",
            "loss: 0.698709  [54416/60000]\n",
            "loss: 0.603868  [56016/60000]\n",
            "loss: 0.605691  [57616/60000]\n",
            "loss: 0.267838  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.0%, Avg loss: 0.536033 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.517201  [   16/60000]\n",
            "loss: 0.539697  [ 1616/60000]\n",
            "loss: 0.474129  [ 3216/60000]\n",
            "loss: 0.512820  [ 4816/60000]\n",
            "loss: 0.442643  [ 6416/60000]\n",
            "loss: 0.691555  [ 8016/60000]\n",
            "loss: 0.603747  [ 9616/60000]\n",
            "loss: 0.371860  [11216/60000]\n",
            "loss: 0.468760  [12816/60000]\n",
            "loss: 0.365033  [14416/60000]\n",
            "loss: 0.972248  [16016/60000]\n",
            "loss: 0.446243  [17616/60000]\n",
            "loss: 0.356548  [19216/60000]\n",
            "loss: 0.846817  [20816/60000]\n",
            "loss: 0.409902  [22416/60000]\n",
            "loss: 0.829923  [24016/60000]\n",
            "loss: 0.434911  [25616/60000]\n",
            "loss: 0.809735  [27216/60000]\n",
            "loss: 0.933653  [28816/60000]\n",
            "loss: 0.504969  [30416/60000]\n",
            "loss: 0.649289  [32016/60000]\n",
            "loss: 0.218582  [33616/60000]\n",
            "loss: 0.317393  [35216/60000]\n",
            "loss: 0.464415  [36816/60000]\n",
            "loss: 0.531140  [38416/60000]\n",
            "loss: 0.803841  [40016/60000]\n",
            "loss: 0.657407  [41616/60000]\n",
            "loss: 0.477147  [43216/60000]\n",
            "loss: 0.579753  [44816/60000]\n",
            "loss: 0.507115  [46416/60000]\n",
            "loss: 0.414605  [48016/60000]\n",
            "loss: 0.371795  [49616/60000]\n",
            "loss: 0.318014  [51216/60000]\n",
            "loss: 0.453581  [52816/60000]\n",
            "loss: 0.497942  [54416/60000]\n",
            "loss: 0.510672  [56016/60000]\n",
            "loss: 0.652426  [57616/60000]\n",
            "loss: 0.393300  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.6%, Avg loss: 0.524545 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.281525  [   16/60000]\n",
            "loss: 0.411076  [ 1616/60000]\n",
            "loss: 0.643019  [ 3216/60000]\n",
            "loss: 0.730308  [ 4816/60000]\n",
            "loss: 0.535476  [ 6416/60000]\n",
            "loss: 0.782927  [ 8016/60000]\n",
            "loss: 0.596667  [ 9616/60000]\n",
            "loss: 0.364755  [11216/60000]\n",
            "loss: 0.468633  [12816/60000]\n",
            "loss: 0.297913  [14416/60000]\n",
            "loss: 0.749700  [16016/60000]\n",
            "loss: 0.536425  [17616/60000]\n",
            "loss: 0.278931  [19216/60000]\n",
            "loss: 0.801560  [20816/60000]\n",
            "loss: 0.367457  [22416/60000]\n",
            "loss: 0.768844  [24016/60000]\n",
            "loss: 0.598751  [25616/60000]\n",
            "loss: 0.765103  [27216/60000]\n",
            "loss: 0.883021  [28816/60000]\n",
            "loss: 0.503306  [30416/60000]\n",
            "loss: 0.506118  [32016/60000]\n",
            "loss: 0.232760  [33616/60000]\n",
            "loss: 0.355096  [35216/60000]\n",
            "loss: 0.403103  [36816/60000]\n",
            "loss: 0.521946  [38416/60000]\n",
            "loss: 0.679689  [40016/60000]\n",
            "loss: 0.671761  [41616/60000]\n",
            "loss: 0.634005  [43216/60000]\n",
            "loss: 0.311487  [44816/60000]\n",
            "loss: 0.489151  [46416/60000]\n",
            "loss: 0.493434  [48016/60000]\n",
            "loss: 0.375608  [49616/60000]\n",
            "loss: 0.318908  [51216/60000]\n",
            "loss: 0.468215  [52816/60000]\n",
            "loss: 0.509553  [54416/60000]\n",
            "loss: 0.412224  [56016/60000]\n",
            "loss: 0.549855  [57616/60000]\n",
            "loss: 0.474641  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.3%, Avg loss: 0.512958 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.499289  [   16/60000]\n",
            "loss: 0.512399  [ 1616/60000]\n",
            "loss: 0.548914  [ 3216/60000]\n",
            "loss: 0.431213  [ 4816/60000]\n",
            "loss: 0.356714  [ 6416/60000]\n",
            "loss: 0.732358  [ 8016/60000]\n",
            "loss: 0.643766  [ 9616/60000]\n",
            "loss: 0.298530  [11216/60000]\n",
            "loss: 0.264673  [12816/60000]\n",
            "loss: 0.263400  [14416/60000]\n",
            "loss: 0.732526  [16016/60000]\n",
            "loss: 0.365982  [17616/60000]\n",
            "loss: 0.335590  [19216/60000]\n",
            "loss: 0.818018  [20816/60000]\n",
            "loss: 0.369823  [22416/60000]\n",
            "loss: 0.726737  [24016/60000]\n",
            "loss: 0.414276  [25616/60000]\n",
            "loss: 0.842618  [27216/60000]\n",
            "loss: 0.988895  [28816/60000]\n",
            "loss: 0.470673  [30416/60000]\n",
            "loss: 0.450159  [32016/60000]\n",
            "loss: 0.327636  [33616/60000]\n",
            "loss: 0.312599  [35216/60000]\n",
            "loss: 0.521638  [36816/60000]\n",
            "loss: 0.512841  [38416/60000]\n",
            "loss: 0.537178  [40016/60000]\n",
            "loss: 0.724318  [41616/60000]\n",
            "loss: 0.549761  [43216/60000]\n",
            "loss: 0.376047  [44816/60000]\n",
            "loss: 0.468977  [46416/60000]\n",
            "loss: 0.539846  [48016/60000]\n",
            "loss: 0.409304  [49616/60000]\n",
            "loss: 0.267799  [51216/60000]\n",
            "loss: 0.425094  [52816/60000]\n",
            "loss: 0.513554  [54416/60000]\n",
            "loss: 0.673693  [56016/60000]\n",
            "loss: 0.637349  [57616/60000]\n",
            "loss: 0.420528  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.9%, Avg loss: 0.500767 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.335939  [   16/60000]\n",
            "loss: 0.500707  [ 1616/60000]\n",
            "loss: 0.610860  [ 3216/60000]\n",
            "loss: 0.483395  [ 4816/60000]\n",
            "loss: 0.466802  [ 6416/60000]\n",
            "loss: 0.542610  [ 8016/60000]\n",
            "loss: 0.484899  [ 9616/60000]\n",
            "loss: 0.385688  [11216/60000]\n",
            "loss: 0.367634  [12816/60000]\n",
            "loss: 0.241894  [14416/60000]\n",
            "loss: 0.815343  [16016/60000]\n",
            "loss: 0.414491  [17616/60000]\n",
            "loss: 0.278735  [19216/60000]\n",
            "loss: 0.720554  [20816/60000]\n",
            "loss: 0.416182  [22416/60000]\n",
            "loss: 0.794491  [24016/60000]\n",
            "loss: 0.461624  [25616/60000]\n",
            "loss: 0.748808  [27216/60000]\n",
            "loss: 0.776345  [28816/60000]\n",
            "loss: 0.543095  [30416/60000]\n",
            "loss: 0.636531  [32016/60000]\n",
            "loss: 0.290623  [33616/60000]\n",
            "loss: 0.340080  [35216/60000]\n",
            "loss: 0.526617  [36816/60000]\n",
            "loss: 0.381958  [38416/60000]\n",
            "loss: 0.592667  [40016/60000]\n",
            "loss: 0.506837  [41616/60000]\n",
            "loss: 0.544103  [43216/60000]\n",
            "loss: 0.372599  [44816/60000]\n",
            "loss: 0.490927  [46416/60000]\n",
            "loss: 0.362900  [48016/60000]\n",
            "loss: 0.384484  [49616/60000]\n",
            "loss: 0.304235  [51216/60000]\n",
            "loss: 0.581963  [52816/60000]\n",
            "loss: 0.529452  [54416/60000]\n",
            "loss: 0.317791  [56016/60000]\n",
            "loss: 0.604499  [57616/60000]\n",
            "loss: 0.396775  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.491010 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.340665  [   16/60000]\n",
            "loss: 0.579953  [ 1616/60000]\n",
            "loss: 0.577836  [ 3216/60000]\n",
            "loss: 0.439739  [ 4816/60000]\n",
            "loss: 0.426128  [ 6416/60000]\n",
            "loss: 0.753225  [ 8016/60000]\n",
            "loss: 0.631522  [ 9616/60000]\n",
            "loss: 0.332476  [11216/60000]\n",
            "loss: 0.285064  [12816/60000]\n",
            "loss: 0.232836  [14416/60000]\n",
            "loss: 0.758173  [16016/60000]\n",
            "loss: 0.441917  [17616/60000]\n",
            "loss: 0.389570  [19216/60000]\n",
            "loss: 0.668665  [20816/60000]\n",
            "loss: 0.394462  [22416/60000]\n",
            "loss: 0.799254  [24016/60000]\n",
            "loss: 0.478542  [25616/60000]\n",
            "loss: 0.713470  [27216/60000]\n",
            "loss: 0.734450  [28816/60000]\n",
            "loss: 0.509277  [30416/60000]\n",
            "loss: 0.420969  [32016/60000]\n",
            "loss: 0.294912  [33616/60000]\n",
            "loss: 0.294900  [35216/60000]\n",
            "loss: 0.436279  [36816/60000]\n",
            "loss: 0.398890  [38416/60000]\n",
            "loss: 0.600842  [40016/60000]\n",
            "loss: 0.548551  [41616/60000]\n",
            "loss: 0.580412  [43216/60000]\n",
            "loss: 0.379537  [44816/60000]\n",
            "loss: 0.388983  [46416/60000]\n",
            "loss: 0.392097  [48016/60000]\n",
            "loss: 0.353875  [49616/60000]\n",
            "loss: 0.378621  [51216/60000]\n",
            "loss: 0.435097  [52816/60000]\n",
            "loss: 0.469890  [54416/60000]\n",
            "loss: 0.560432  [56016/60000]\n",
            "loss: 0.848627  [57616/60000]\n",
            "loss: 0.453373  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.479611 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.328660  [   16/60000]\n",
            "loss: 0.499095  [ 1616/60000]\n",
            "loss: 0.592606  [ 3216/60000]\n",
            "loss: 0.624689  [ 4816/60000]\n",
            "loss: 0.469094  [ 6416/60000]\n",
            "loss: 0.801151  [ 8016/60000]\n",
            "loss: 0.557124  [ 9616/60000]\n",
            "loss: 0.269805  [11216/60000]\n",
            "loss: 0.449828  [12816/60000]\n",
            "loss: 0.281788  [14416/60000]\n",
            "loss: 0.857864  [16016/60000]\n",
            "loss: 0.359726  [17616/60000]\n",
            "loss: 0.311652  [19216/60000]\n",
            "loss: 0.794692  [20816/60000]\n",
            "loss: 0.339513  [22416/60000]\n",
            "loss: 0.757799  [24016/60000]\n",
            "loss: 0.520922  [25616/60000]\n",
            "loss: 0.674970  [27216/60000]\n",
            "loss: 0.771598  [28816/60000]\n",
            "loss: 0.566516  [30416/60000]\n",
            "loss: 0.615347  [32016/60000]\n",
            "loss: 0.246320  [33616/60000]\n",
            "loss: 0.362608  [35216/60000]\n",
            "loss: 0.466259  [36816/60000]\n",
            "loss: 0.255102  [38416/60000]\n",
            "loss: 0.670901  [40016/60000]\n",
            "loss: 0.728343  [41616/60000]\n",
            "loss: 0.592472  [43216/60000]\n",
            "loss: 0.477044  [44816/60000]\n",
            "loss: 0.519634  [46416/60000]\n",
            "loss: 0.566584  [48016/60000]\n",
            "loss: 0.401218  [49616/60000]\n",
            "loss: 0.286401  [51216/60000]\n",
            "loss: 0.462461  [52816/60000]\n",
            "loss: 0.429045  [54416/60000]\n",
            "loss: 0.395075  [56016/60000]\n",
            "loss: 0.607915  [57616/60000]\n",
            "loss: 0.381379  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.4%, Avg loss: 0.471500 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.316250  [   16/60000]\n",
            "loss: 0.652600  [ 1616/60000]\n",
            "loss: 0.536927  [ 3216/60000]\n",
            "loss: 0.393353  [ 4816/60000]\n",
            "loss: 0.381175  [ 6416/60000]\n",
            "loss: 0.866530  [ 8016/60000]\n",
            "loss: 0.487611  [ 9616/60000]\n",
            "loss: 0.292246  [11216/60000]\n",
            "loss: 0.356144  [12816/60000]\n",
            "loss: 0.253785  [14416/60000]\n",
            "loss: 0.867308  [16016/60000]\n",
            "loss: 0.324605  [17616/60000]\n",
            "loss: 0.244083  [19216/60000]\n",
            "loss: 0.777086  [20816/60000]\n",
            "loss: 0.360342  [22416/60000]\n",
            "loss: 0.808991  [24016/60000]\n",
            "loss: 0.322028  [25616/60000]\n",
            "loss: 0.686172  [27216/60000]\n",
            "loss: 0.931965  [28816/60000]\n",
            "loss: 0.538853  [30416/60000]\n",
            "loss: 0.439320  [32016/60000]\n",
            "loss: 0.185463  [33616/60000]\n",
            "loss: 0.491068  [35216/60000]\n",
            "loss: 0.389481  [36816/60000]\n",
            "loss: 0.498917  [38416/60000]\n",
            "loss: 0.559604  [40016/60000]\n",
            "loss: 0.439269  [41616/60000]\n",
            "loss: 0.603790  [43216/60000]\n",
            "loss: 0.447308  [44816/60000]\n",
            "loss: 0.395712  [46416/60000]\n",
            "loss: 0.381529  [48016/60000]\n",
            "loss: 0.317281  [49616/60000]\n",
            "loss: 0.251347  [51216/60000]\n",
            "loss: 0.431246  [52816/60000]\n",
            "loss: 0.649592  [54416/60000]\n",
            "loss: 0.275713  [56016/60000]\n",
            "loss: 0.511334  [57616/60000]\n",
            "loss: 0.435079  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.462155 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.336172  [   16/60000]\n",
            "loss: 0.542939  [ 1616/60000]\n",
            "loss: 0.639719  [ 3216/60000]\n",
            "loss: 0.338218  [ 4816/60000]\n",
            "loss: 0.470913  [ 6416/60000]\n",
            "loss: 0.692366  [ 8016/60000]\n",
            "loss: 0.473791  [ 9616/60000]\n",
            "loss: 0.267652  [11216/60000]\n",
            "loss: 0.286495  [12816/60000]\n",
            "loss: 0.316627  [14416/60000]\n",
            "loss: 0.939181  [16016/60000]\n",
            "loss: 0.327684  [17616/60000]\n",
            "loss: 0.270229  [19216/60000]\n",
            "loss: 0.671914  [20816/60000]\n",
            "loss: 0.326993  [22416/60000]\n",
            "loss: 0.796765  [24016/60000]\n",
            "loss: 0.449498  [25616/60000]\n",
            "loss: 0.810500  [27216/60000]\n",
            "loss: 0.802579  [28816/60000]\n",
            "loss: 0.481644  [30416/60000]\n",
            "loss: 0.453903  [32016/60000]\n",
            "loss: 0.212788  [33616/60000]\n",
            "loss: 0.390054  [35216/60000]\n",
            "loss: 0.339380  [36816/60000]\n",
            "loss: 0.595252  [38416/60000]\n",
            "loss: 0.610268  [40016/60000]\n",
            "loss: 0.698912  [41616/60000]\n",
            "loss: 0.812637  [43216/60000]\n",
            "loss: 0.590941  [44816/60000]\n",
            "loss: 0.442235  [46416/60000]\n",
            "loss: 0.352888  [48016/60000]\n",
            "loss: 0.357368  [49616/60000]\n",
            "loss: 0.274614  [51216/60000]\n",
            "loss: 0.406282  [52816/60000]\n",
            "loss: 0.517233  [54416/60000]\n",
            "loss: 0.373865  [56016/60000]\n",
            "loss: 0.633504  [57616/60000]\n",
            "loss: 0.420014  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.454307 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.506148  [   16/60000]\n",
            "loss: 0.581220  [ 1616/60000]\n",
            "loss: 0.575044  [ 3216/60000]\n",
            "loss: 0.412803  [ 4816/60000]\n",
            "loss: 0.406035  [ 6416/60000]\n",
            "loss: 0.733747  [ 8016/60000]\n",
            "loss: 0.477052  [ 9616/60000]\n",
            "loss: 0.334454  [11216/60000]\n",
            "loss: 0.366969  [12816/60000]\n",
            "loss: 0.222476  [14416/60000]\n",
            "loss: 0.764737  [16016/60000]\n",
            "loss: 0.390372  [17616/60000]\n",
            "loss: 0.285033  [19216/60000]\n",
            "loss: 0.645053  [20816/60000]\n",
            "loss: 0.338049  [22416/60000]\n",
            "loss: 0.661556  [24016/60000]\n",
            "loss: 0.408209  [25616/60000]\n",
            "loss: 0.624273  [27216/60000]\n",
            "loss: 0.996349  [28816/60000]\n",
            "loss: 0.438450  [30416/60000]\n",
            "loss: 0.524569  [32016/60000]\n",
            "loss: 0.142307  [33616/60000]\n",
            "loss: 0.369644  [35216/60000]\n",
            "loss: 0.410587  [36816/60000]\n",
            "loss: 0.483421  [38416/60000]\n",
            "loss: 0.483654  [40016/60000]\n",
            "loss: 0.612997  [41616/60000]\n",
            "loss: 0.575634  [43216/60000]\n",
            "loss: 0.441078  [44816/60000]\n",
            "loss: 0.449711  [46416/60000]\n",
            "loss: 0.397254  [48016/60000]\n",
            "loss: 0.428481  [49616/60000]\n",
            "loss: 0.243168  [51216/60000]\n",
            "loss: 0.518122  [52816/60000]\n",
            "loss: 0.473555  [54416/60000]\n",
            "loss: 0.451091  [56016/60000]\n",
            "loss: 0.532303  [57616/60000]\n",
            "loss: 0.313265  [59216/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.8%, Avg loss: 0.444831 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "batch_size=16\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=16)\n",
        "test_dataloader = DataLoader(test_data, batch_size=16)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.CNN_relu_stack = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=4),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, 1, 28, 28)\n",
        "            dummy_output = self.CNN_relu_stack(dummy_input)\n",
        "            flatten_dim = dummy_output.view(1, -1).shape[1]\n",
        "            print(f'A= {flatten_dim}')\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(flatten_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 10)  # 10 output classes for FashionMNIST\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.CNN_relu_stack(x)\n",
        "        x = self.flatten(x)\n",
        "        logits = self.classifier(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()\n",
        "\n",
        "learning_rate = 1e-3\n",
        "epochs = 20\n",
        "\n",
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss() # Since, the output is more than 2 possible values, Cross Entropy was prefered.\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Using Stochastic Gradient Descent for optimising weights and biases after finding losses.\n",
        "\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad() # reinitialise gradient to 0\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 20\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get a single image and label from the test dataset\n",
        "image, label = test_data[1]\n",
        "\n",
        "# Add a batch dimension: [1, 1, 28, 28]\n",
        "image_batch = image.unsqueeze(0)\n",
        "\n",
        "# Set model to eval mode and get prediction\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(image_batch)\n",
        "    predicted_class = output.argmax(1).item()\n",
        "\n",
        "# Show the image and prediction\n",
        "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "plt.title(f\"Predicted: {predicted_class} | Actual: {label}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "QxqU4AfksmEa",
        "outputId": "2a9c21d8-316c-49ad-9270-7f6e61d5a614"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}